{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT Summarization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeXY15FwEqz7"
      },
      "source": [
        "##Text summarization with BERT\n",
        "\n",
        "Code adopted from: https://huggingface.co/blog/warm-starting-encoder-decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEcH-sNSBYNl"
      },
      "source": [
        "is_colab = True"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubN4Yxys4lNU"
      },
      "source": [
        "##Import packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-9b4unzBtzJ"
      },
      "source": [
        "Install packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehOahbuUBTIx"
      },
      "source": [
        "if is_colab:\n",
        "    !pip install datasets\n",
        "    !pip install rouge_score\n",
        "    !pip install transformers==4.5.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cXw4N7bB0Ss"
      },
      "source": [
        "Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENp61KyiRojE"
      },
      "source": [
        "import os\n",
        "\n",
        "import datasets\n",
        "from datasets import load_dataset, load_metric\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import (BertTokenizerFast, EncoderDecoderModel,\n",
        "                          Seq2SeqTrainer, Seq2SeqTrainingArguments)\n",
        "\n",
        "if is_colab:\n",
        "    from google.colab import drive"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQ0iVwQ5uj-i"
      },
      "source": [
        "##Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71ULNudMBCTC"
      },
      "source": [
        "if is_colab:\n",
        "    # Load data from Google Drive\n",
        "    drive.mount('/content/drive')\n",
        "    data_folder = '/content/drive/MyDrive/deep_learning/project/data/cxr'\n",
        "\n",
        "    train_path = os.path.join(data_folder, \"train.csv\")\n",
        "    val_path = os.path.join(data_folder, \"validation.csv\")\n",
        "    test_path = os.path.join(data_folder, \"test.csv\")\n",
        "\n",
        "else:\n",
        "    # Load data from local directory\n",
        "    data_folder = \"/home/labuser/Documents/deep_learning/project/data/cxr\"\n",
        "\n",
        "    train_path = os.path.join(data_folder, \"train.csv\")\n",
        "    val_path = os.path.join(data_folder, \"validation.csv\")\n",
        "    test_path = os.path.join(data_folder, \"test.csv\")\n",
        "\n",
        "train_data = load_dataset('csv', data_files=train_path, split='train')\n",
        "val_data = load_dataset('csv', data_files=val_path, split='train')\n",
        "test_data = load_dataset('csv', data_files=test_path, split='train')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVqAJeMe40H_"
      },
      "source": [
        "##Tokenize data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i97i1eDl446P"
      },
      "source": [
        "def map_to_length(x):\n",
        "    \"\"\"\n",
        "    Generates summary statistics for data.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : Dataset\n",
        "        Dataframe with text and summary columns.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    x : Dataset\n",
        "        Dataframe with additional columns showing data statistics.\n",
        "    \"\"\"\n",
        "\n",
        "    x[\"text_len\"] = len(tokenizer(x[\"text\"]).input_ids)\n",
        "    x[\"text_longer_128\"] = int(x[\"text_len\"] > 128)\n",
        "    x[\"text_longer_256\"] = int(x[\"text_len\"] > 256)\n",
        "    x[\"summary_len\"] = len(tokenizer(x[\"summary\"]).input_ids)\n",
        "    x[\"summary_longer_64\"] = int(x[\"summary_len\"] > 64)\n",
        "    x[\"summary_longer_128\"] = int(x[\"summary_len\"] > 128)\n",
        "    return x\n",
        "\n",
        "\n",
        "def compute_and_print_stats(x, sample_size=10000):\n",
        "    \"\"\"\n",
        "    Print summary statistics for data.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : Dataset\n",
        "        Dataset returned from `map_to_length`.\n",
        "    sample_size : int\n",
        "        Number of samples to be included in calculation.\n",
        "    \"\"\"\n",
        "\n",
        "    if len(x[\"summary_len\"]) == sample_size:\n",
        "        print(\"Text Mean: {:.3f}, %-Text > 128: {:.3f}, %-Text > 256: {:.3f}\\n\"\\\n",
        "              \"Summary Mean: {:.3f}, %-Summary > 64: {:.3f}, %-Summary > 128: {:.3f}\".format(\n",
        "                sum(x[\"text_len\"]) / sample_size,\n",
        "                sum(x[\"text_longer_128\"]) / sample_size * 100,\n",
        "                sum(x[\"text_longer_256\"]) / sample_size * 100,\n",
        "                sum(x[\"summary_len\"]) / sample_size,\n",
        "                sum(x[\"summary_longer_64\"]) / sample_size * 100,\n",
        "                sum(x[\"summary_longer_128\"]) / sample_size * 100,\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "def tokenize_batch(batch, enc_max_len=256, dec_max_len=128):\n",
        "    \"\"\"\n",
        "    Tokenize a batch of data.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    batch : Dataset\n",
        "        Batch of dataset.\n",
        "    enc_max_len : int\n",
        "        Maximum input length for encoder.\n",
        "    dec_max_len : int\n",
        "        Maximum input length for decoder.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    batch : Dataset\n",
        "        Dataframe with tokenized data.\n",
        "    \"\"\"\n",
        "\n",
        "    # tokenize inputs and outputs\n",
        "    x = tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True,\n",
        "                  max_length=enc_max_len)\n",
        "    y = tokenizer(batch[\"summary\"], padding=\"max_length\", truncation=True,\n",
        "                  max_length=dec_max_len)\n",
        "    \n",
        "    # include info in dict\n",
        "    batch[\"input_ids\"] = x.input_ids\n",
        "    batch[\"attention_mask\"] = x.attention_mask\n",
        "    batch[\"decoder_input_ids\"] = y.input_ids\n",
        "    batch[\"decoder_attention_mask\"] = y.attention_mask\n",
        "    batch[\"labels\"] = y.input_ids.copy()\n",
        "\n",
        "    # ignore PAD token\n",
        "    batch[\"labels\"] = [[-100 if token == tokenizer.pad_token_id\n",
        "                        else token for token in labels]\n",
        "                       for labels in batch[\"labels\"]]\n",
        "    return batch\n",
        "\n",
        "\n",
        "def tokenize_data(data):\n",
        "    \"\"\"\n",
        "    Tokenize data in batches.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data : Dataset\n",
        "        Dataframe with text and summary columns.\n",
        "    \n",
        "    Returns##\n",
        "    -------\n",
        "    data : Dataset\n",
        "        Dataframe with tokenized data.\n",
        "    \"\"\"\n",
        "\n",
        "    data = data.map(\n",
        "        tokenize_batch,\n",
        "        batched=True,\n",
        "        batch_size=16,\n",
        "        remove_columns=[\"study_id\", \"subject_id\", \"text\", \"summary\"]\n",
        "    )\n",
        "    data.set_format(\n",
        "        type=\"torch\",\n",
        "        columns=[\"input_ids\", \"attention_mask\",\n",
        "                 \"decoder_input_ids\", \"decoder_attention_mask\",\n",
        "                 \"labels\"]\n",
        "    )\n",
        "    return data"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76yGuAM4Gurg"
      },
      "source": [
        "Initialize tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WktH3PCUz4U0"
      },
      "source": [
        "tokenizer_name = \"dmis-lab/biobert-base-cased-v1.1\"\n",
        "#tokenizer_name = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained(tokenizer_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-0ds0e0HBy4"
      },
      "source": [
        "Compute data statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0WW242H0aza"
      },
      "source": [
        "data_stats = train_data.select(range(10000)).map(map_to_length, num_proc=4)\n",
        "output = data_stats.map(compute_and_print_stats, batched=True, batch_size=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQwnVNheHIi4"
      },
      "source": [
        "Tokenize data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcPzGQ7r7DVT"
      },
      "source": [
        "train_data_tokenized = tokenize_data(train_data)\n",
        "val_data_tokenized = tokenize_data(val_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08CLLvac9KDu"
      },
      "source": [
        "##Initialize model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWGSCmZEHLG4"
      },
      "source": [
        "Select encoder and decoder, and whether to share parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSo8Q-6h-JZQ"
      },
      "source": [
        "enc_name = \"dmis-lab/biobert-base-cased-v1.1\"\n",
        "dec_name = \"dmis-lab/biobert-base-cased-v1.1\"\n",
        "tie_encoder_decoder=False\n",
        "model_name = \"biobert2biobert\"\n",
        "\n",
        "#enc_name = \"dmis-lab/biobert-base-cased-v1.1\"\n",
        "#dec_name = \"dmis-lab/biobert-base-cased-v1.1\"\n",
        "#tie_encoder_decoder=True\n",
        "#model_name = \"biobertshare\"\n",
        "\n",
        "#enc_name = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
        "#dec_name = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
        "#tie_encoder_decoder=False\n",
        "#model_name = \"clinicalbert2clinicalbert\"\n",
        "\n",
        "#enc_name = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
        "#dec_name = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
        "#tie_encoder_decoder=True\n",
        "#model_name = \"clinicalbertshare\""
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFLdtWNVHPQw"
      },
      "source": [
        "Initialize model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFvm3oFQ9Jho"
      },
      "source": [
        "model = EncoderDecoderModel.from_encoder_decoder_pretrained(enc_name, dec_name,\n",
        "                                                            tie_encoder_decoder=tie_encoder_decoder)\n",
        "\n",
        "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
        "model.config.eos_token_id = tokenizer.sep_token_id\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "model.config.vocab_size = model.config.encoder.vocab_size\n",
        "\n",
        "model.config.max_length = 142\n",
        "model.config.min_length = 56\n",
        "model.config.no_repeat_ngram_size = 3\n",
        "model.config.early_stopping = True\n",
        "model.config.length_penalty = 2.0\n",
        "model.config.num_beams = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_f-Vkej4rN-"
      },
      "source": [
        "##Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOWIugvPBLxM"
      },
      "source": [
        "def compute_metrics(pred):\n",
        "    \"\"\"\n",
        "    Compute ROUGE score for predicted summary.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    pred : \n",
        "        Predicted tokenized summary.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        ROUGE-2 precision, recall and fmeasure.\n",
        "    \"\"\"\n",
        "\n",
        "    labels_ids = pred.label_ids\n",
        "    pred_ids = pred.predictions\n",
        "\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
        "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
        "\n",
        "    rouge_output = rouge.compute(predictions=pred_str, references=label_str,\n",
        "                                 rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
        "\n",
        "    return {\n",
        "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
        "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
        "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
        "    }\n",
        "\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    predict_with_generate=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    fp16=False, \n",
        "    output_dir=\"./\",\n",
        "    logging_steps=1000,\n",
        "    save_steps=500,\n",
        "    eval_steps=7500,\n",
        "    warmup_steps=2000,\n",
        "    save_total_limit=3,\n",
        ")\n",
        "\n",
        "rouge = load_metric(\"rouge\")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_data_tokenized,\n",
        "    eval_dataset=val_data_tokenized,\n",
        ")"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYrEcBJRBCmy"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}